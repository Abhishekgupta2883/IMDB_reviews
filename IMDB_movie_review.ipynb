{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzw-uMJIR58B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGDw-WMFSYjD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv', engine='python', on_bad_lines='skip')\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy5V8wICSq25"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "x84Q_UvZSvYA"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eLzHP1Q2Szb7"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lPmxdD4aTSUM"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvJWG5RlTYmQ"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAc2UcnDTibM"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw0aw5puXXEq"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "caNzW1IAVjvR"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qioNkSOnWxR7"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aqs1FQ6fXEB5"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBb09XkvWz-Y"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6bqAdDgVwwJ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def transform_text (text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'<br />', '', text) # Remove <br /> tags\n",
        "  text = nltk.word_tokenize(text)\n",
        "  y = []\n",
        "  for i in text:\n",
        "    if i.isalnum():\n",
        "      y.append(i)\n",
        "  text = y[:]\n",
        "  y.clear()\n",
        "  for i in text:\n",
        "    if i not in stopwords.words('english') and i not in string.punctuation:\n",
        "       y.append(i)\n",
        "\n",
        "  text = y[:]\n",
        "  y.clear()\n",
        "  for i in text:\n",
        "    y.append(ps.stem(i))\n",
        "  return \" \".join(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQEthfpIXF3X"
      },
      "outputs": [],
      "source": [
        "df[\"transformed_text\"] = df[\"review\"].apply(transform_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5cb7c5c"
      },
      "outputs": [],
      "source": [
        "df.to_csv('imdb_dataset_cleaned.csv', index=False)\n",
        "print('DataFrame exported to imdb_dataset_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRhF29a8r4dF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCpIsJ2tb2ad"
      },
      "outputs": [],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiCOxxladt3l"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vppR1j8rd4L5"
      },
      "outputs": [],
      "source": [
        "good_review=wc.generate(df[df['sentiment']==1]['transformed_text'].str.cat(sep=\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7nF4E7VZeApT"
      },
      "outputs": [],
      "source": [
        "plt.imshow(good_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UA3CofBefOB"
      },
      "outputs": [],
      "source": [
        "bad_review=wc.generate(df[df['sentiment']==0]['transformed_text'].str.cat(sep=\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SRAW4qINelDr"
      },
      "outputs": [],
      "source": [
        "plt.imshow(bad_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U0VDpV_fCHv"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQoKIt0kki0b"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features= 5000)\n",
        "X = tfidf.fit_transform(df['transformed_text']).toarray()\n",
        "display(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cRVn_aIq9dYs"
      },
      "outputs": [],
      "source": [
        "pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DtxUDq6_9fBm"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b3RpYR6L9jGZ"
      },
      "outputs": [],
      "source": [
        "X = model.encode(\n",
        "    df['transformed_text'].tolist(),\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17VDjqoyjvd4"
      },
      "outputs": [],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cjcxBomjxPn"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjnzyDAHj8hw"
      },
      "outputs": [],
      "source": [
        "sentences = df['transformed_text'].apply(lambda x: x.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTz-4dYjkSO_"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    workers=4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL9gwzC4kaQ8"
      },
      "outputs": [],
      "source": [
        "def document_vector(doc):\n",
        "    words = doc.split()\n",
        "    vectors = [\n",
        "        w2v.wv[word]\n",
        "        for word in words\n",
        "        if word in w2v.wv\n",
        "    ]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v.vector_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhy9SQ7xkb5X"
      },
      "outputs": [],
      "source": [
        "X = np.vstack(df['transformed_text'].apply(document_vector))\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGSqrQ2PklmC"
      },
      "outputs": [],
      "source": [
        "y = df['sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5x1toR6kuRK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Dlo5bfk3IF"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ds5d3ZH4k4J5"
      },
      "outputs": [],
      "source": [
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqOt3W2xk64M"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "h_HudKYjr7eg"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1_IuDq3r_gE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score\n",
        "\n",
        "def objective(trial):\n",
        "    # 2. Define ngram_range parameter\n",
        "    ngram_range = trial.suggest_categorical('ngram_range', [(1, 1), (1, 2)])\n",
        "\n",
        "    # 3. Define max_df parameter\n",
        "    max_df = trial.suggest_float('max_df', 0.5, 0.8, step=0.05)\n",
        "\n",
        "    # 4. Define min_df parameter\n",
        "    min_df = trial.suggest_int('min_df', 1, 5)\n",
        "\n",
        "    # 5. Define max_features parameter\n",
        "    max_features = trial.suggest_int('max_features', 4000, 6000, step=200)\n",
        "\n",
        "    # 6. Initialize TfidfVectorizer with trial parameters\n",
        "    tfidf = TfidfVectorizer(ngram_range=ngram_range, max_df=max_df, min_df=min_df, max_features=max_features)\n",
        "\n",
        "    # 7. Apply TfidfVectorizer\n",
        "    X = tfidf.fit_transform(df['transformed_text']).toarray()\n",
        "\n",
        "    # Prepare target variable\n",
        "    y = df['sentiment'].values\n",
        "\n",
        "    # 8. Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "    # 10. Train the model\n",
        "    lr = LogisticRegression(solver='liblinear') # Using 'liblinear' solver for binary classification\n",
        "    lr.fit(X_train, y_train)\n",
        "\n",
        "    # 11. Make predictions\n",
        "    y_pred = lr.predict(X_test)\n",
        "\n",
        "    # Calculate F1-score and Precision\n",
        "    current_f1_score = f1_score(y_test, y_pred)\n",
        "\n",
        "    return current_f1_score\n",
        "\n",
        "print(\"Objective function 'objective' redefined to prioritize precision=1 successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zhTlemTvsK6r"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "# Create an Optuna study object\n",
        "# We want to maximize the F1-score among precision=1 solutions, so direction is 'maximize'\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Run the optimization process with the new objective function\n",
        "# Call the objective function for a specified number of trials (e.g., 100)\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Print the best trial's value (maximum F1-score achieved with precision=1)\n",
        "print(f\"Best trial's F1-score : {study.best_value:.4f}\")\n",
        "\n",
        "# Print the best trial's parameters\n",
        "print(\"Best trial's parameters (for precision=1):\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrSlCwGDsS5U"
      },
      "outputs": [],
      "source": [
        "best_params_precision1 = study.best_params\n",
        "\n",
        "optimal_ngram_range_precision1 = best_params_precision1['ngram_range']\n",
        "optimal_max_df_precision1 = best_params_precision1['max_df']\n",
        "optimal_min_df_precision1 = best_params_precision1['min_df']\n",
        "optimal_max_features_precision1 = best_params_precision1['max_features']\n",
        "\n",
        "print(\"Stored optimal TF-IDF parameters for precision=1:\")\n",
        "print(f\"  ngram_range: {optimal_ngram_range_precision1}\")\n",
        "print(f\"  max_df: {optimal_max_df_precision1}\")\n",
        "print(f\"  min_df: {optimal_min_df_precision1}\")\n",
        "print(f\"  max_features: {optimal_max_features_precision1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20JNSFcWZ2ON"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(  ngram_range =(1, 1), max_df= 0.55, min_df= 2, max_features= 5200)\n",
        "X = tfidf.fit_transform(df['transformed_text']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62235fd0"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# Define the filename for the exported TF-IDF vectorizer\n",
        "tfidf_filename = 'tfidf_vectorizer.pkl'\n",
        "\n",
        "# Open the file in binary write mode and save the tfidf object\n",
        "with open(tfidf_filename, 'wb') as file:\n",
        "    pickle.dump(tfidf, file)\n",
        "\n",
        "print(f\"TF-IDF vectorizer exported to {tfidf_filename}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2574223"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "lr = LogisticRegression(\n",
        "        solver=\"liblinear\",\n",
        "        penalty=\"l2\",\n",
        "        C=2.909456422800902,\n",
        "        max_iter=195,\n",
        "        class_weight=\"balanced\",\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
        "print(f\"Logistic Regression Precision: {precision_score(y_test, y_pred_lr):.4f}\")\n",
        "print(f\"Logistic Regression Recall: {recall_score(y_test, y_pred_lr):.4f}\")\n",
        "print(f\"Logistic Regression F1 Score: {f1_score(y_test, y_pred_lr):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SemWeF6ua7FO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "421489c1"
      },
      "outputs": [],
      "source": [
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Logistic Regression Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68554468"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objective_lr(trial):\n",
        "\n",
        "    # Regularization strength\n",
        "    C = trial.suggest_float(\"C\", 1e-3, 10.0, log=True)\n",
        "\n",
        "    # Tolerance\n",
        "    tol = trial.suggest_float(\"tol\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "    # Class weight\n",
        "    class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
        "\n",
        "    # Max iterations\n",
        "    max_iter = trial.suggest_int(\"max_iter\", 200, 1000)\n",
        "\n",
        "    # Build model\n",
        "    model = LogisticRegression(\n",
        "        solver = \"liblinear\",\n",
        "        penalty = \"l2\", # Added missing comma here\n",
        "        C=C,\n",
        "        tol=tol,\n",
        "        class_weight=class_weight,\n",
        "        max_iter=max_iter,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Metric\n",
        "    return f1_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Objective function 'objective_lr' redefined successfully with comprehensive hyperparameter tuning for Logistic Regression.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ea801447"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "# Create an Optuna study object to maximize the F1-score\n",
        "study_lr = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Run the optimization process with the objective_lr function for 100 trials\n",
        "study_lr.optimize(objective_lr, n_trials=50)\n",
        "\n",
        "# Print the best trial's value (maximum F1-score)\n",
        "print(f\"Best trial's F1-score for Logistic Regression: {study_lr.best_value:.4f}\")\n",
        "\n",
        "# Print the best trial's parameters\n",
        "print(\"Best trial's parameters for Logistic Regression:\")\n",
        "for key, value in study_lr.best_params.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj7zrc29vThw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ef35b6f"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Instantiate an XGBClassifier object\n",
        "xgb = XGBClassifier(random_state=2, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# Calculate and print the evaluation metrics\n",
        "print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"XGBoost Precision: {precision_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"XGBoost Recall: {recall_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"XGBoost F1 Score: {f1_score(y_test, y_pred_xgb):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc87e7d1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Re-split the data after updating X with optimal TF-IDF parameters\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Instantiate an XGBClassifier object\n",
        "xgb = XGBClassifier(random_state=2, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# Calculate and print the evaluation metrics\n",
        "print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"XGBoost Precision: {precision_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"XGBoost Recall: {recall_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(f\"XGBoost F1 Score: {f1_score(y_test, y_pred_xgb):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05f718d2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate a RandomForestClassifier object\n",
        "rf = RandomForestClassifier(random_state=2)\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Calculate and print the evaluation metrics\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "print(f\"Random Forest Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
        "print(f\"Random Forest Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
        "print(f\"Random Forest F1 Score: {f1_score(y_test, y_pred_rf):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e4bae54"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Instantiate an SVC object\n",
        "# Using a linear kernel for text data can be effective and computationally less expensive than RBF for high-dimensional sparse data\n",
        "svm = SVC(kernel='linear', random_state=2)\n",
        "\n",
        "# Train the SVM model\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "# Calculate and print the evaluation metrics\n",
        "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
        "print(f\"SVM Precision: {precision_score(y_test, y_pred_svm):.4f}\")\n",
        "print(f\"SVM Recall: {recall_score(y_test, y_pred_svm):.4f}\")\n",
        "print(f\"SVM F1 Score: {f1_score(y_test, y_pred_svm):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d41e5290"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Instantiate a KNeighborsClassifier object\n",
        "# Using n_neighbors=5 as a common starting point\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the KNN model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Calculate and print the evaluation metrics\n",
        "print(f\"KNN Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
        "print(f\"KNN Precision: {precision_score(y_test, y_pred_knn):.4f}\")\n",
        "print(f\"KNN Recall: {recall_score(y_test, y_pred_knn):.4f}\")\n",
        "print(f\"KNN F1 Score: {f1_score(y_test, y_pred_knn):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}